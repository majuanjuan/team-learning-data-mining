{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "### What is boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is the idea of using multiple weak classifiers to generate strong classifiers.\n",
    "\n",
    "The first round of training in the lift model uses different subsets of samples to train the base classifier. After training, the first round of the base classifier is used to classify and obtain the results of the first round of classification. Continue training after weighting samples that were misclassified in the first round of classification results. Repeat the n-wheel and the fusion model that the training will eventually get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three step in adaboost:\n",
    "\n",
    "1. Initialize the weight distribution of the training data\n",
    "\n",
    "\n",
    "2. Train with a training dataset with weight distribution to get the basic classifier.\n",
    "   \n",
    "  Calculate the classification error rate on the training dataset\n",
    "  \n",
    "  Update the weight distribution of the training dataset. \n",
    "  \n",
    "  \n",
    "3. Creates a linear combination of classifiers with weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An adaboost implement by python:\n",
    "\n",
    "ref:https://www.jianshu.com/p/ef16be5b66f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseClassify(dataMartix, dim, threshVal, threshIneq):\n",
    "    ret = ones((shape(dataMaritx)[0],1))\n",
    "    if threshIneq=='lt':#less\n",
    "        ret[dataMatrix[:,dim]<=threshVal]=-1.0\n",
    "    else:\n",
    "        ret[dataMatrix[:,dim]>threshVal]=-1.0\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildStump(dataArr,classLabels,D):\n",
    "#将数据集和标签列表转为矩阵形式\n",
    "    dataMatrix=mat(dataArr);labelMat=mat(classLabels).T\n",
    "    m,n=shape(dataMatrix)\n",
    "    numSteps=10.0;bestStump={};bestClasEst=mat(zeros((m,1))) \n",
    "    #最小错误率初始化为+∞\n",
    "    minError=inf\n",
    "    \n",
    "    #特征->特征的步长->阈值对比方式\n",
    "    for i in range(n):\n",
    "        rangeMin=dataMatrix[:,i].min();rangeMax=dataMatrix[:,i].max()\n",
    "        stepSize=(rangeMax-rangeMin)/numSteps\n",
    "        for j in range(-1,int(numSteps)+1):\n",
    "            for inequal in ['lt','gt']:\n",
    "                threshVal=rangeMin+float(j)*stepSize\n",
    "                predictedVals=\\\n",
    "                    stumpClassify(dataMatrix,i,threshVal,inequal)\n",
    "                errArr=mat(ones((m,1)))\n",
    "                errArr[predictedVals==labelMat]=0\n",
    "                #计算\"加权\"的错误率\n",
    "                #这里终于看懂了！！\n",
    "                weigthedError=D.T*errArr\n",
    "                if weigthedError<minError:\n",
    "                    minError=weigthedError\n",
    "                    bestClasEst=predictedVals.copy()\n",
    "                    bestStump['dim']=i\n",
    "                    bestStump['thresh']=threshVal\n",
    "                    bestStump['ineq']=inequal\n",
    "    #返回最佳单层决策树相关信息的字典，最小错误率，决策树预测输出结果\n",
    "    return bestStump,minError,bestClasEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr,classLabels,numIt=40):\n",
    "    weakClassArr=[];m=shape(dataArr)[0]\n",
    "    D=mat(ones((m,1))/m) #初始化样本权重\n",
    "    aggClassEst=mat(zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump,error,classEst=buildStump(dataArr,classLabels,D)\n",
    "        alpha = float(0.5*log((1.0-error)/max(error,1e-16))) #max(error,1e-16)防止0误差的计算溢出\n",
    "        bestStump[\"alpha\"]=alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "        #print (\"D:\",D.T,\"\\n\",\"predClass:\",classEst.T)\n",
    "        \n",
    "        #为下一次迭代更新D(很关键，矩阵运算容易写错！)\n",
    "        expon=multiply(-1*alpha*mat(classLabels).T,classEst) #shape(5,1)*shape(5,1)对应元素相乘，得到各样本的-alpha*yi*G(xi)\n",
    "        D=multiply(D,exp(expon)) #shape(5,1)*shape(5,1)对应元素相乘，\n",
    "        D=D/D.sum() #得到各样本更新后的wi\n",
    "        \n",
    "        #计算汇集的分类结果aggClassEst(即，将每次最佳决策树分类结果bestClass*alpha相加)\n",
    "        aggClassEst+=alpha*classEst\n",
    "        #print(\"aggClassEst:\",aggClassEst.T)\n",
    "        aggErrors=multiply(sign(aggClassEst)!=mat(classLabels).T,ones((m,1)))\n",
    "        errorRate=aggErrors.sum()/m\n",
    "        #print(\"total error:\",errorRate)\n",
    "        if errorRate==0.0:\n",
    "            break\n",
    "    return weakClassArr,errorRate,aggClassEst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
