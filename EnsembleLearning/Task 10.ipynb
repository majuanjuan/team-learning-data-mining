{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10\n",
    "\n",
    "In the last task, the idea of boosting model was introduced, which is to generate the final model through the combination of multiple weak classifiers, and this model is generally better than a single weak classifier. But we have not yet known how the boosting algorithm of multiple weak classifiers is designed, so this task includes the following parts:\n",
    "\n",
    "\n",
    "1. Introduction to forward distribution algorithm\n",
    "\n",
    "2. The similarities and differences between Xgboost and LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward distribution algorithm\n",
    "\n",
    "The core idea of the forward distribution algorithm is very similar to the greedy algorithm. \n",
    "\n",
    "As the same as the greedy algorithm, the forward distribution algorithm learns only one weak model and its coefficients at each step, so that the current weak model and all previous weak models are combined to obtain the optimal target expression value, which can finally make the target expression obtain the optimal value after all the weak models are combined.\n",
    "\n",
    "AdaBoost is a special case of the forward distribution addition algorithm. At this time, the model is an addition model composed of basic classifiers, and the loss function is an exponential function\n",
    "\n",
    "ref:https://www.jianshu.com/p/0c1dfcfcb92b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The similarities and differences between Xgboost and LightGBM\n",
    "\n",
    "\n",
    "|        |          XGBOOST     |     LightGBM     |\n",
    "|-----------|----------------------------|----------------------|\n",
    "|Complexity   | XGBoost uses a decision tree algorithm based on pre-sorting. It needs to calculate the feature gain every time a feature is traversed, and the time complexity is O(datafeature) | LightGBM uses a histogram-based decision tree algorithm. The histogram optimization algorithm only needs to be calculated K times, and the time complexity is O(Kfeature) |\n",
    "|Base classifier (Tree)    |level-wise|leaf-wise|\n",
    "|Support to categorical feature| No | Yes |\n",
    "\n",
    "\n",
    "A detailed compare is introduced in this blog:\n",
    "https://www.zybuluo.com/Team/note/1095836#5-lightgbm%E4%B8%8Exgboost%E5%AF%B9%E6%AF%94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
